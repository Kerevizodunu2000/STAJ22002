{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMRdf6HBR7PfaMbh4mj7pft"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["https://github.com/bentrevett/pytorch-sentiment-analysis?tab=readme-ov-file\n","\n","https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/main/1%20-%20Neural%20Bag%20of%20Words.ipynb#scrollTo=a5478fc3\n","\n","https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/main/2%20-%20Recurrent%20Neural%20Networks.ipynb\n","\n","https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/main/3%20-%20Convolutional%20Neural%20Networks.ipynb\n","\n","https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/main/4%20-%20Transformers.ipynb"],"metadata":{"id":"YgiNn8xD4k6n"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eFQtC-R83Mhu","executionInfo":{"status":"ok","timestamp":1725360135231,"user_tz":-180,"elapsed":3162,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}},"outputId":"a8de725b-da3c-450c-bc07-2aa25335d6e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#Collap Drive Bağlama\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Uninstall existing versions\n","!pip uninstall torch torchtext -y\n","\n","# Install compatible versions of torch and torchtext\n","!pip install torch==2.2.0 torchtext==0.17.0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BHJcDRKvZLpT","executionInfo":{"status":"ok","timestamp":1725360122472,"user_tz":-180,"elapsed":89113,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}},"outputId":"5ad36e6e-93df-40fd-a339-8828099eeb7c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.4.0\n","Uninstalling torch-2.4.0:\n","  Successfully uninstalled torch-2.4.0\n","Found existing installation: torchtext 0.18.0\n","Uninstalling torchtext-0.18.0:\n","  Successfully uninstalled torchtext-0.18.0\n","Collecting torch==2.2.0\n","  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting torchtext==0.17.0\n","  Downloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.6.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.0.106)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n","Collecting triton==2.2.0 (from torch==2.2.0)\n","  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (4.66.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (2.32.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (1.26.4)\n","Collecting torchdata==0.7.1 (from torchtext==0.17.0)\n","  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.6.68)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.0.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n","Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, torchdata, torchtext\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.0.0\n","    Uninstalling triton-3.0.0:\n","      Successfully uninstalled triton-3.0.0\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.20.5\n","    Uninstalling nvidia-nccl-cu12-2.20.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n","    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 2.2.0 which is incompatible.\n","torchvision 0.19.0+cu121 requires torch==2.4.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 torch-2.2.0 torchdata-0.7.1 torchtext-0.17.0 triton-2.2.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","torchgen"]},"id":"14a57bc3e91c44c3afb1e3b226928041"}},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","print(torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SeP8FpdAdVhp","executionInfo":{"status":"ok","timestamp":1725359822871,"user_tz":-180,"elapsed":7,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}},"outputId":"db5d99dd-fcb1-4568-ca01-0f8680535605"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["2.4.0+cu121\n"]}]},{"cell_type":"code","source":["import torch\n","import torchtext\n","print(torch.__version__)\n","print(torchtext.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E6s-pAiWdagT","executionInfo":{"status":"ok","timestamp":1725360139706,"user_tz":-180,"elapsed":4479,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}},"outputId":"0d5641ac-95dd-4911-8716-5894b24758fc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.0+cu121\n","0.17.0+cpu\n"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","import pandas as pd\n","import re\n","import nltk\n","from sklearn.metrics import accuracy_score\n","\n","# Download necessary NLTK packages\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Define custom stop words based on your setup\n","stop_words = set(nltk.corpus.stopwords.words(\"turkish\"))\n","stop_words.update([\"bir\", \"film\", \"filmi\", \"filme\", \"filmde\", \"filmden\", \"filmin\", \"kadar\", \"bi\", \"ben\"])  # Add your custom stop words"],"metadata":{"id":"p5tz4nMJ4ULf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725360740426,"user_tz":-180,"elapsed":1056,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}},"outputId":"75845cc9-c0b9-4671-d76b-7cfb0049510f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["# Define the tokenizer and text preprocessing function\n","def preprocess_text(text):\n","    text = text.lower()  # Convert to lowercase\n","    text = re.sub(\"[^abcçdefgğhıijklmnoöprsştuüvyz ]\", \"\", text)  # Remove non-Turkish alphabetic characters\n","    tokens = nltk.word_tokenize(text)  # Tokenize the text\n","    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n","    lemma = nltk.WordNetLemmatizer()  # Initialize lemmatizer\n","    tokens = [lemma.lemmatize(word) for word in tokens]  # Lemmatize tokens\n","    return tokens\n","\n","# Define a function to yield tokens for building the vocabulary\n","def yield_tokens(data_iter):\n","    for text, _ in data_iter:\n","        yield preprocess_text(text)"],"metadata":{"id":"XQcx5MVYYgW9","executionInfo":{"status":"ok","timestamp":1725360229489,"user_tz":-180,"elapsed":1012,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Define the path to the datasets\n","path = \"/content/drive/MyDrive/Binovist  Sentiment Analysis/data/Hepsiburada Dataset 492k(%10 test)/\"\n","\n","# Load the train and test datasets\n","df_train = pd.read_csv(path + \"train.csv\")\n","df_test = pd.read_csv(path + \"test.csv\")\n","\n","# Combine datasets into a dictionary for easier processing\n","dataframes = {'df_train': df_train, 'df_test': df_test}\n","\n","# Define sets for affirmative and negative responses (if needed in interactive filtering processes)\n","yes_responses = {'evet', 'yes', 'y', '1', 'tamam', 'onay', 'olur', 'e'}\n","no_responses = {'hayır', 'no', 'n', '0', 'hayir', 'olmaz', 'h'}"],"metadata":{"id":"B4-cSh2vFqsf","executionInfo":{"status":"ok","timestamp":1725360761420,"user_tz":-180,"elapsed":11516,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Map labels to numeric values\n","label_mapping = {'Positive': 2, 'Notr': 1, 'Negative': 0}\n","\n","# Apply the label mapping to both datasets\n","for name, df in dataframes.items():\n","    # Ensure the label column exists and map it to numeric values\n","    if 'label' in df.columns:\n","        df['label_numeric'] = df['label'].map(label_mapping)\n","        print(f\"{name} Hatasız Eklendi\")\n","    else:\n","        print(f\"Label column not found in {name}\")\n","\n","# Display the first few rows of the datasets to confirm changes\n","print(\"Train Dataset Sample:\")\n","print(df_train.head())\n","\n","print(\"\\nTest Dataset Sample:\")\n","print(df_test.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JWBwEfsg-ac","executionInfo":{"status":"ok","timestamp":1725360763693,"user_tz":-180,"elapsed":477,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}},"outputId":"a64f7356-b8e0-47e9-e4d7-4928dacab60d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["df_train Hatasız Eklendi\n","df_test Hatasız Eklendi\n","Train Dataset Sample:\n","                                                text     label  \\\n","0  ürünü hepsiburadadan alalı 3 hafta oldu. orjin...  Positive   \n","1  ürünlerden çok memnunum, kesinlikle herkese ta...  Positive   \n","2      hızlı kargo, temiz alışveriş.teşekkür ederim.  Positive   \n","3               Çünkü aranan tapınak bu bölgededir .      Notr   \n","4  bu telefonu başlıca alma nedenlerim ise elimde...  Positive   \n","\n","          dataset  label_numeric  \n","0  urun_yorumlari              2  \n","1  urun_yorumlari              2  \n","2  urun_yorumlari              2  \n","3            wiki              1  \n","4  urun_yorumlari              2  \n","\n","Test Dataset Sample:\n","                                                text     label  \\\n","0      Kral akbaba dikkat çekici renklere sahiptir .      Notr   \n","1   ısrarla korkutmayı başarıyor. sanki korku çok...  Positive   \n","2  Neşe ve Üzüntü köprünün kırılmaya başlamasıyla...      Notr   \n","3  i phone 5 ten sonra gene 4'' ekranı tercih ett...  Positive   \n","4    Beşinci sezonda diziye yeni oyuncular katıldı .      Notr   \n","\n","          dataset  label_numeric  \n","0            wiki              1  \n","1           HUMIR              2  \n","2            wiki              1  \n","3  urun_yorumlari              2  \n","4            wiki              1  \n"]}]},{"cell_type":"code","source":["# Build the vocabulary using the training data\n","vocab = build_vocab_from_iterator(yield_tokens(df_train), specials=[\"<unk>\", \"<pad>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])"],"metadata":{"id":"C92KwsiMhOmp","executionInfo":{"status":"error","timestamp":1725360835140,"user_tz":-180,"elapsed":488,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}},"outputId":"b90970b4-03e2-4543-8f1c-d86a06273d9a","colab":{"base_uri":"https://localhost:8080/","height":315}},"execution_count":13,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-873d99e58fc6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the vocabulary using the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myield_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/vocab/vocab_factory.py\u001b[0m in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-e6292303fa46>\u001b[0m in \u001b[0;36myield_tokens\u001b[0;34m(data_iter)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Define a function to yield tokens for building the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0myield_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","source":["# Define the model architecture\n","class SentimentAnalysisModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n","        super(SentimentAnalysisModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n","                            bidirectional=bidirectional, dropout=dropout, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text):\n","        embedded = self.dropout(self.embedding(text))\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden = self.dropout(torch.cat((lstm_out[:, -1, :self.lstm.hidden_size],\n","                                         lstm_out[:, 0, self.lstm.hidden_size:]), dim=1))\n","        return self.fc(hidden)"],"metadata":{"id":"-vhMngFBYmPh","executionInfo":{"status":"aborted","timestamp":1725358641158,"user_tz":-180,"elapsed":7,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model parameters\n","INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 3  # 3 classes: Positive, Neutral, Negative\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"],"metadata":{"id":"eFOT3ZK5Yn69","executionInfo":{"status":"aborted","timestamp":1725358641158,"user_tz":-180,"elapsed":6,"user":{"displayName":"Kerevizodunu 2000","userId":"06694509492921643392"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the model\n","model = SentimentAnalysisModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n","\n","# Load pre-trained embeddings\n","model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters())\n","\n","# Move model to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","criterion = criterion.to(device)"],"metadata":{"id":"BcUygOV0Yu6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to train the model\n","def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    for batch in iterator:\n","        optimizer.zero_grad()\n","        predictions = model(batch.text).squeeze(1)\n","        loss = criterion(predictions, batch.label)\n","        acc = accuracy_score(batch.label.cpu(), predictions.argmax(1).cpu())\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"],"metadata":{"id":"dfu6Tc6kY2vE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to evaluate the model\n","def evaluate(model, iterator, criterion):\n","    model.eval()\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    predictions_list = []\n","    labels_list = []\n","\n","    with torch.no_grad():\n","        for batch in iterator:\n","            predictions = model(batch.text).squeeze(1)\n","            loss = criterion(predictions, batch.label)\n","            acc = accuracy_score(batch.label.cpu(), predictions.argmax(1).cpu())\n","            epoch_loss += loss.item()\n","            epoch_acc += acc\n","            predictions_list.extend(predictions.argmax(1).cpu().numpy())\n","            labels_list.extend(batch.label.cpu().numpy())\n","\n","    print(classification_report(labels_list, predictions_list, target_names=['Negative', 'Neutral', 'Positive']))\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"],"metadata":{"id":"-1bcoxbKY4s6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","N_EPOCHS = 5\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n","\n","    print(f'Epoch: {epoch+1:02}')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","# Saving the trained model\n","torch.save(model.state_dict(), 'sentiment_model_pytorch.pth')\n","print(\"Model training completed and saved.\")"],"metadata":{"id":"a927d4hLY6-N"},"execution_count":null,"outputs":[]}]}